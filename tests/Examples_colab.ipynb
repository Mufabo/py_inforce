{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Examples-checkpoint.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mufabo/py_inforce/blob/master/tests/Examples_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2pqigmj9Qmq",
        "colab_type": "text"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfu9vLoY9Tk9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "4b119b6a-8d00-4e32-ff2c-ad8f1e1993af"
      },
      "source": [
        "!git clone https://github.com/Mufabo/py_inforce.git\n",
        "%cd py_inforce\n",
        "!pip install -e ."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'py_inforce'...\n",
            "remote: Enumerating objects: 128, done.\u001b[K\n",
            "remote: Counting objects: 100% (128/128), done.\u001b[K\n",
            "remote: Compressing objects: 100% (87/87), done.\u001b[K\n",
            "remote: Total 128 (delta 47), reused 108 (delta 33), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (128/128), 644.77 KiB | 872.00 KiB/s, done.\n",
            "Resolving deltas: 100% (47/47), done.\n",
            "/content/py_inforce\n",
            "Obtaining file:///content/py_inforce\n",
            "Installing collected packages: py-inforce\n",
            "  Running setup.py develop for py-inforce\n",
            "Successfully installed py-inforce\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKHowQc46bZv",
        "colab_type": "text"
      },
      "source": [
        "# Dynamic Programming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNlXH7m76bZz",
        "colab_type": "text"
      },
      "source": [
        "## Policy Iteration on FrozenLake"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lNXuGLc6bZ3",
        "colab_type": "code",
        "colab": {},
        "outputId": "1826f481-d245-4cca-e973-0b05652f6476"
      },
      "source": [
        "import py_inforce as pin\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "env = gym.make('FrozenLake-v0', is_slippery=True)  \n",
        "\n",
        "policy = pin.policy_iteration(env, .95, thresh=0.00001)\n",
        "\n",
        "returns = []\n",
        "\n",
        "for i in range(100):\n",
        "    state = env.reset()\n",
        "\n",
        "    done = False\n",
        "    ret = 0\n",
        "\n",
        "    while not done:\n",
        "        action = np.where(policy[state, :] == 1)\n",
        "        state, reward, done, _ = env.step(action[0][0])\n",
        "        ret += reward\n",
        "    returns.append(ret)\n",
        "    \n",
        "sum(returns)/100"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.78"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KD1494Qw6baa",
        "colab_type": "text"
      },
      "source": [
        "## Value Iteration on FrozenLake"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzNc-N036bac",
        "colab_type": "code",
        "colab": {},
        "outputId": "7597b210-1801-4add-9334-bae3e3e1baa0"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import py_inforce as pin\n",
        "\n",
        "env = gym.make('FrozenLake-v0', is_slippery=True)  \n",
        "\n",
        "policy, _ = pin.value_iteration(env)\n",
        "\n",
        "returns = []\n",
        "\n",
        "for i in range(100):\n",
        "    state = env.reset()\n",
        "\n",
        "    done = False\n",
        "    ret = 0\n",
        "\n",
        "    while not done:\n",
        "        action = np.where(policy[state, :] == 1)\n",
        "        state, reward, done, _ = env.step(action[0][0])\n",
        "        ret += reward\n",
        "    returns.append(ret)\n",
        "    \n",
        "sum(returns)/100"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.84"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aycTrRxV6bap",
        "colab_type": "text"
      },
      "source": [
        "# Policy Gradient Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLSWHnx36bar",
        "colab_type": "text"
      },
      "source": [
        "## REINFORCE on Cartpole"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vr17gpgW6bav",
        "colab_type": "code",
        "colab": {},
        "outputId": "cccf7df1-28df-47a3-9b9c-d117e8a9e4c7"
      },
      "source": [
        "from torch.distributions import Categorical\n",
        "import gym\n",
        "import torch.nn as nn\n",
        "from py_inforce.generic.mlp import MLP\n",
        "from py_inforce.policy_based.REINFORCE import REINFORCE\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import numpy as np\n",
        "import py_inforce as pin\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "in_dim = env.observation_space.shape[0] # 4\n",
        "out_dim = env.action_space.n # 2\n",
        "cart_agent = MLP([in_dim, 128, 128, out_dim], nn.ReLU)\n",
        "optimizer = optim.Adam(cart_agent.parameters(), lr=cart_agent.lr)\n",
        "\n",
        "# Stops when the agent achieves a score of 200 just once\n",
        "pin.REINFORCE(cart_agent, env, Categorical, optimizer, 200, bf = lambda x: x - x.mean(), MAX_EPISODES=500, EARLY = lambda x: x == 200)\n",
        "\n",
        "done = False\n",
        "\n",
        "state = env.reset()\n",
        "rewards = 0\n",
        "\n",
        "while not done:\n",
        "    state = torch.from_numpy(state.astype(np.float32))\n",
        "    pd = Categorical(logits=cart_agent.forward(state))\n",
        "    action = pd.sample()\n",
        "    state, reward, done, _ = env.step(action.numpy())\n",
        "    rewards += reward\n",
        "    #env.render()\n",
        "    \n",
        "rewards"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqO6AUAI6ba7",
        "colab_type": "text"
      },
      "source": [
        "# Temporal Difference Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LfrPoGX6ba9",
        "colab_type": "text"
      },
      "source": [
        "## SARSA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXpbEv8W8-hK",
        "colab_type": "text"
      },
      "source": [
        "# Value Based Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkCAjsj09C93",
        "colab_type": "text"
      },
      "source": [
        "## Deep Q Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjT3VvAe9Fkk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "90cdc80c-1d40-4f70-9ac8-ab3fb127c28f"
      },
      "source": [
        "import gym\n",
        "import torch.nn as nn\n",
        "from py_inforce.generic.mlp import MLP\n",
        "from py_inforce.value_based.DQN import DQN\n",
        "from py_inforce.generic.Memories import ReplayMemory\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "in_dim = env.observation_space.shape[0] # 4\n",
        "out_dim = env.action_space.n # 2\n",
        "q_net = MLP([in_dim, 128, 128, out_dim], nn.ReLU, LEARN_RATE = 0.005)\n",
        "t_net = MLP([in_dim, 128, 128, out_dim], nn.ReLU, LEARN_RATE = 0.005)\n",
        "optimizer = optim.Adam\n",
        "memory = ReplayMemory(1000, in_dim, out_dim)\n",
        "\n",
        "DQN(env, memory, q_net, optimizer, steps = 10000, eps = 1, disc_factor = 0.99, loss = torch.nn.MSELoss(), batch_sz = 32)\n",
        "# Note: This was very lucky"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "converged in 42 steps\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le0oG87p9Jtt",
        "colab_type": "text"
      },
      "source": [
        "## Double Deep Q Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxGvOPdj9MCC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "070305e9-04d8-43f8-e149-ca6bae7fc934"
      },
      "source": [
        "import gym\n",
        "import torch.nn as nn\n",
        "from py_inforce.generic.mlp import MLP\n",
        "from py_inforce.value_based.Double_DQN import Double_DQN\n",
        "from py_inforce.generic.Memories import ReplayMemory\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "in_dim = env.observation_space.shape[0] # 4\n",
        "out_dim = env.action_space.n # 2\n",
        "q_net = MLP([in_dim, 128, 128, out_dim], nn.ReLU, LEARN_RATE = 0.005)\n",
        "t_net = MLP([in_dim, 128, 128, out_dim], nn.ReLU)\n",
        "optimizer = optim.Adam\n",
        "memory = ReplayMemory(1000, in_dim, out_dim)\n",
        "\n",
        "Double_DQN(env, memory, q_net, t_net, optimizer, steps = 10000, eps = 1, disc_factor = 0.99, loss = torch.nn.MSELoss(), batch_sz = 32, tgt_update = 100)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "converged in 1459 steps\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}